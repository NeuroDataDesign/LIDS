{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/drishtimannan/anaconda3/envs/bloby/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tifffile\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv of the ground truth as a pandas dataframe. Here we are extracting centroid locations for all timepoints. This will be used to train and test LDA performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Z    Y    X\n",
      "0     0  136   81\n",
      "1     0  122  105\n",
      "2     0   64   84\n",
      "3     4   81   80\n",
      "4     1  106   87\n",
      "5     0   88   96\n",
      "6     1   64  116\n",
      "7     0   29  102\n",
      "8     0   13   96\n",
      "9     1   18  113\n",
      "10    3   62  131\n",
      "11    1  107  128\n",
      "12    2   97  134\n",
      "13    5  104  127\n",
      "14    5   93  138\n",
      "15    5   80  105\n",
      "16    5   80  118\n",
      "17    6   67  122\n",
      "18    7   77  142\n",
      "19    5   56  108\n",
      "20    2  105  131\n",
      "21    2  115  123\n",
      "22   10   63  125\n",
      "23    9   53  153\n",
      "24    8   49  116\n",
      "25    9   32  118\n",
      "26   10   48  113\n",
      "27   13   50   74\n",
      "28   13   59   72\n",
      "29   13   86   86\n",
      "..   ..  ...  ...\n",
      "350  10  108   42\n",
      "351   3  113  121\n",
      "352   8  156   43\n",
      "353   4   16  112\n",
      "354   1   13   65\n",
      "355  14   54  123\n",
      "356   9  145  127\n",
      "357   7   83  119\n",
      "358   4  133   72\n",
      "359   2  142   35\n",
      "360  15   95  137\n",
      "361   1   64  127\n",
      "362   0   63  102\n",
      "363   3   41   53\n",
      "364   1  117   99\n",
      "365  15   42  158\n",
      "366  10   22  151\n",
      "367   7   32  127\n",
      "368   4    8  101\n",
      "369   9   39   60\n",
      "370  10  134   92\n",
      "371  12   32  136\n",
      "372  14  138  120\n",
      "373   6   79   88\n",
      "374   7  139   81\n",
      "375   9  144   28\n",
      "376   7   40   41\n",
      "377   0   55   74\n",
      "378   6   48  126\n",
      "379  13  119  178\n",
      "\n",
      "[380 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "##upload ground truth labels for generating training data for LDA\n",
    "# ground1=pd.read_csv('../dmannan1/LDA2.1/Substacks_with_SpineROIs/a1_tp1.csv')\n",
    "# ground2 = pd.read_csv('../dmannan1/LDA2.1/Substacks_with_SpineROIs/a2_tp2.csv')\n",
    "# ground3 = pd.read_csv(\"../dmannan1/LDA2.1/Substacks_with_SpineROIs/a3_tp3.csv\")\n",
    "# ground4 = pd.read_csv('../dmannan1/LDA2.1/Substacks_with_SpineROIs/a4_tp4.csv')\n",
    "test_g = pd.read_csv(\"../dmannan1/LDA2/ground_truth.csv\")\n",
    "# #print(len(ground1))\n",
    "# print(len(ground2))\n",
    "# print(len(ground3))\n",
    "# print(len(ground4))\n",
    "print(test_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change the panadas dataframe to numpy array and arrange the columns as Z,Y,X\n",
    "\n",
    "# ground_truth1 = np.asarray(ground1[['Z', 'Y', 'X']])\n",
    "# ground_truth2 = np.asarray(ground2[['Z', 'Y', 'X']])\n",
    "# ground_truth3 = np.asarray(ground3[['Z', 'Y', 'X']])\n",
    "# ground_truth4 = np.asarray(ground4[['Z', 'Y', 'X']])\n",
    "test_g1 = np.asarray(test_g[['Z', 'Y', 'X']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The csv given, the z-slices range from 1-16, but need to range from 0-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ground_truth1[:,0] = ground_truth1[:,0] - 1 ## -1 to make sure the dummy array index and ground_truth match\n",
    "# ground_truth2[:,0] = ground_truth2[:,0] - 1\n",
    "# ground_truth3[:,0] = ground_truth3[:,0] - 1\n",
    "# ground_truth4[:,0] = ground_truth4[:,0] - 1\n",
    "\n",
    "#print(ground_truth1)\n",
    "#print(ground_truth2)\n",
    "#print(ground_truth3)\n",
    "#print(ground_truth4)\n",
    "# len(ground_truth1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make sure that synapse cubes do not overlap with non-synapses in training and testing set\n",
    "Extract centroid locations of known synapses and mark an area around the synapses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_array = np.zeros((16, 359, 359))##to keep track of the centroids and the area around it to make sure training \n",
    "##set doesn't have any overlapping areas. We need 359x359 by 16 arrays. In here provide the dimensions of tiff file \n",
    "## you are working with in z,y,x format\n",
    "# dummy_array.shape\n",
    "# dummy_array[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_test = np.zeros((16,200,200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally need to extract non-synapses that don't overlap with synapse locations or their cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def centroid_cubes(size_cube, array, centroid_locations): ##need to make cubes around our synapses to ensure they don't overlap with \n",
    "##non-synapses\n",
    "\n",
    "    sin=0 \n",
    "    for z, y ,x in zip(centroid_locations[:,0], centroid_locations[:,1], centroid_locations[:,2]):\n",
    "        if x-size_cube<0 or x+size_cube>199 or y-size_cube<0 or y+size_cube>199:\n",
    "            continue\n",
    "        for l in range(y-size_cube,y+size_cube): ## 4.84 microns cubed cube is formed by a 22x22 rectangle around the centroid locations\n",
    "            for m in range(x-size_cube,x+size_cube):\n",
    "                array[z][l][m] = 1\n",
    "        sin = sin+1\n",
    "        \n",
    "\n",
    "    return  array, sin\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa, ss =centroid_cubes(11,dummy_test,test_g1)\n",
    "type(ss)\n",
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finding_non_synapses(size_cube, sin, array2):        \n",
    "    num = 0 \n",
    "    z_loc = []\n",
    "    y_loc = []\n",
    "    x_loc = []\n",
    "    \n",
    "    while num < sin:\n",
    "    \n",
    "    \n",
    "        x_ns = np.random.randint(size_cube,200-size_cube)##discarding points within 5 pixels from boundary of 359x359x16 \n",
    "    ##pixels because want to generate a 4.84 cube for mean intensity. Also, x and y values range from 0-358.\n",
    "        y_ns = np.random.randint(size_cube,200-size_cube)\n",
    "        z_ns = np.random.randint(0,16)\n",
    "        if array2[z_ns][y_ns][x_ns] ==1 or array2[z_ns][y_ns-size_cube][x_ns-size_cube]==1 or array2[z_ns][y_ns+size_cube][x_ns+size_cube]==1 or array2[z_ns][y_ns-size_cube][x_ns+size_cube]==1 or array2[z_ns][y_ns+size_cube][x_ns-size_cube]==1:\n",
    "            continue\n",
    "        z_loc.append(z_ns)\n",
    "        y_loc.append(y_ns)\n",
    "        x_loc.append(x_ns)\n",
    "        for s in range(y_ns-size_cube,y_ns+size_cube): ## 4.84 microns cubed cube is formed by a 10x10 rectangle around the centroid locations\n",
    "            for p in range(x_ns-size_cube,x_ns+size_cube):\n",
    "                 array2[z_ns][s][p] = 1\n",
    "        num += 1\n",
    "        print(num)\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    return np.asarray((z_loc,y_loc,x_loc)).T\n",
    "  \n",
    "    \n",
    "\n",
    "#updated_dummy = finding_non_syn(5, ground_truth1, dummy_array) ##this is the new non_syn training set\n",
    "\n",
    "#updated_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "[[  7 183 136]\n",
      " [ 14 152  55]\n",
      " [  3 165 109]\n",
      " ..., \n",
      " [  7 116  96]\n",
      " [  4  43 107]\n",
      " [ 12  72 114]]\n"
     ]
    }
   ],
   "source": [
    "non_syn = finding_non_synapses(11,345,aa)\n",
    "print(non_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def finding_non_syn_test(size_cube, centroid_locations, array): ##need to make cubes around our synapses to ensure they don't overlap with \n",
    "# ##non-synapses\n",
    "     \n",
    "#     try: \n",
    "#         for z, y ,x in zip(centroid_locations[:,0], centroid_locations[:,1], centroid_locations[:,2]):\n",
    "#             for l in range(y-size_cube,y+size_cube): ## 4.84 microns cubed cube is formed by a 22x22 rectangle around the centroid locations\n",
    "#                 for m in range(x-size_cube,x+size_cube):\n",
    "#                     array[z][l][m] = 1\n",
    "#     except:\n",
    "#         pass\n",
    "        \n",
    "#     num = 0 \n",
    "#     z_loc = []\n",
    "#     y_loc = []\n",
    "#     x_loc = []\n",
    "#     while num < len(centroid_locations):\n",
    "    \n",
    "#         x_ns = np.random.randint(size_cube,200-size_cube)##discarding points within 5 pixels from boundary of 359x359x16 \n",
    "#     ##pixels because want to generate a 4.84 cube for mean intensity. Also, x and y values range from 0-358.\n",
    "#         y_ns = np.random.randint(size_cube,200-size_cube)\n",
    "#         z_ns = np.random.randint(0,16)\n",
    "#         if array[z_ns][y_ns][x_ns] ==1 or array[z_ns][y_ns-size_cube][x_ns-size_cube]==1 or array[z_ns][y_ns+size_cube][x_ns+size_cube]==1 or array[z_ns][y_ns-size_cube][x_ns+size_cube]==1 or array[z_ns][y_ns+size_cube][x_ns-size_cube]==1:\n",
    "#             continue\n",
    "#         z_loc.append(z_ns)\n",
    "#         y_loc.append(y_ns)\n",
    "#         x_loc.append(x_ns)\n",
    "#         for s in range(y_ns-size_cube,y_ns+size_cube): ## 4.84 microns cubed cube is formed by a 10x10 rectangle around the centroid locations\n",
    "#             for p in range(x_ns-size_cube,x_ns+size_cube):\n",
    "#                 array[z_ns][s][p] = 1\n",
    "#         num += 1\n",
    "        \n",
    "\n",
    "# test_up=finding_non_syn_test(11,test_g1,dummy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the image tiff stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im1 = tifffile.imread('Substacks_with_SpineROIs/R04_tp1_substack (17-32).tif')\n",
    "im_test = tifffile.imread('../dmannan1/LDA2/lids_smallest_stack.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally finding the mean intensity of the cubes around the centroids generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_intensity(samples, cube_size, image):\n",
    "    all_intensity = []\n",
    "    try:\n",
    "        \n",
    "        for z, y, x in zip(samples[:,0], samples[:,1], samples[:,2]):##take the z,x,y locations to iterate over\n",
    "            intensity = []\n",
    "            \n",
    "            for l in range(y-cube_size,y+cube_size): ## 1 microns cubed cube is formed by a 22x22 rectangle around the centroid locations\n",
    "                for m in range(x-cube_size,x+cube_size):\n",
    "                    intensity.append(image[z,l,m])\n",
    "            \n",
    "            mean_intensity = np.mean(intensity)\n",
    "            all_intensity.append(mean_intensity)\n",
    "            print(all_intensity)\n",
    "    except:\n",
    "        pass\n",
    "    return all_intensity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating intensities for synapses and non-synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a =centroid_cubes(11,test_g1)\n",
    "# u = finding_non_synapses(11,s,a)\n",
    "# synt = generating_intensity(test_g1,11,im_test)\n",
    "# nst = generating_intensity(u,11,im_test)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_in= generating_intensity(ground_truth1, 5)\n",
    "\n",
    "syn_intensity = np.array(syn_in).reshape((222,1)) ## reshape the list to be a 2D array\n",
    "syn_intensity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_syn_in= generating_intensity(updated_dummy, 5)\n",
    "non_syn_intensity = np.array(non_syn_in).reshape((222,1))\n",
    "non_syn_intensity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the intensity arrays into training and testing:\n",
    "Decide how much the intensity set will be split into testing and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##training set, need the 1st 74 rows choosing 33% of the data to be training set\n",
    "train_syn = syn_intensity[:111]\n",
    "train_non_syn = non_syn_intensity[:111]\n",
    "    \n",
    "\n",
    "    \n",
    "##Testing set, take next rows \n",
    "test_syn= syn_intensity[111:]\n",
    "test_non_syn = non_syn_intensity[111:]\n",
    "\n",
    "    \n",
    "    \n",
    "### The final training set:\n",
    "train_data = np.vstack((train_syn, train_non_syn))\n",
    "test_data = np.vstack((test_syn, test_non_syn))\n",
    "### The final label generation, synapses belong to class 1, non-synpases belong to class 0:\n",
    "train_labels = np.append(np.ones((len(train_syn))), np.zeros(len(train_non_syn)))\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_syn.shape)\n",
    "print(test_syn.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = []\n",
    "p = []\n",
    "r = []\n",
    "f = []\n",
    "prediction_int_overlays = []\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    \n",
    "    lda = LDA().fit(train_data, train_labels)\n",
    "    probs_positive_class = lda.predict_proba(test_data)[:, 1]\n",
    "\n",
    "    prediction = probs_positive_class > i\n",
    "    prediction_int = prediction.astype(int)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "       \n",
    "    wrong_syn = 148-sum(prediction_int[:148]) ##know that the 1st 220 are synapses\n",
    "    wrong_non_syn=sum(prediction_int[148:]) ###know that the last 220 are non-synapses\n",
    "    precision = sum(prediction_int[:148])/ (sum(prediction_int[:220])+wrong_non_syn)\n",
    "    recall = sum(prediction_int[:148])/ (sum(prediction_int[:148])+(148-sum(prediction_int[:148])))\n",
    "    if precision and recall != 0:\n",
    "        f1 = (2*precision*recall)/(precision + recall)\n",
    "    else:\n",
    "        f1 = 0 \n",
    "    print(prediction_int, \"Total wrong synapses:\", wrong_syn, \"Total wrong non-synapses:\", wrong_non_syn, \"Precision:\",\n",
    "         precision, \"Recall:\", recall, \"F1\", f1)\n",
    "    p.append(precision)\n",
    "    r.append(recall)\n",
    "    b.append(i)\n",
    "    f.append(f1)\n",
    "    prediction_int_overlays.append(prediction_int)\n",
    "    \n",
    "plt.plot(b, p)\n",
    "plt.plot(b, r)\n",
    "plt.plot(b, f)\n",
    "plt.xlabel('Decision boundary threshold')\n",
    "plt.ylabel('Value of the labelled metric')\n",
    "\n",
    "plt.legend(['Precision', 'Recall', \"F1\"], loc='upper right')\n",
    "plt.title('Mean intensity of a 4.84 ${\\mu}m^3$ cube and LDA performance by varying decision threshold')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Running it on timpeoint 1\n",
    "\n",
    "lda = LDA().fit(train_data, train_labels)\n",
    "probs_positive_class = lda.predict_proba(test_data)[:,1]\n",
    "\n",
    "prediction = probs_positive_class > 0.\n",
    "prediction_int = prediction.astype(int)\n",
    "print(prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LDA()\n",
    "clf.fit(train_data, train_labels)\n",
    "#predict_proba(test_data)\n",
    "j = clf.predict(test_data)\n",
    "print(j)\n",
    "print(clf.predict_proba(test_data))\n",
    "print(clf.predict_proba(test_data)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_positive_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(j[148:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
