{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/drishtimannan/anaconda3/envs/LIDS/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tifffile\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the csv of the ground truth as a pandas dataframe. Here we are extracting centroid locations for all timepoints. This will be used to train and test LDA performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "195\n",
      "218\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "#upload ground truth labels for generating training data for LDA\n",
    "ground1 = pd.read_csv('../dmannan1/LDA2/Substacks_with_SpineROIs/a1_tp1.csv')\n",
    "ground2 = pd.read_csv('../dmannan1/LDA2/Substacks_with_SpineROIs/a2_tp2.csv')\n",
    "ground3 = pd.read_csv(\"../dmannan1/LDA2/Substacks_with_SpineROIs/a3_tp3.csv\")\n",
    "ground4 = pd.read_csv('../dmannan1/LDA2/Substacks_with_SpineROIs/a4_tp4.csv')\n",
    "\n",
    "print(len(ground1))\n",
    "print(len(ground2))\n",
    "print(len(ground3))\n",
    "print(len(ground4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change the panadas dataframe to numpy array and arrange the columns as Z,Y,X\n",
    "\n",
    "ground_truth1 = np.asarray(ground1[['Z', 'Y', 'X']])\n",
    "ground_truth2 = np.asarray(ground2[['Z', 'Y', 'X']])\n",
    "ground_truth3 = np.asarray(ground3[['Z', 'Y', 'X']])\n",
    "ground_truth4 = np.asarray(ground4[['Z', 'Y', 'X']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The csv given, the z-slices range from 1-16, but need to range from 0-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "195\n",
      "218\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "ground_truth1[:,0] = ground_truth1[:,0] - 1 ## -1 to make sure the dummy array index and ground_truth match\n",
    "ground_truth2[:,0] = ground_truth2[:,0] - 1\n",
    "ground_truth3[:,0] = ground_truth3[:,0] - 1\n",
    "ground_truth4[:,0] = ground_truth4[:,0] - 1\n",
    "\n",
    "# print(ground_truth1)\n",
    "# print(ground_truth2)\n",
    "# print(ground_truth3)\n",
    "# print(ground_truth4)\n",
    "print(len(ground_truth1))\n",
    "print(len(ground_truth2))\n",
    "print(len(ground_truth3))\n",
    "print(len(ground_truth4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make sure that synapse cubes do not overlap with non-synapses in training and testing set\n",
    "Extract centroid locations of known synapses and mark an area around the synapses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_array1 = np.zeros((16, 359, 359))##to keep track of the centroids and the area around it to make sure training \n",
    "#set doesn't have any overlapping areas. We need 359x359 by 16 arrays. In here provide the dimensions of tiff file \n",
    "# you are working with in z,y,x format\n",
    "dummy_array2 = np.zeros((16, 359, 359))\n",
    "dummy_array3 = np.zeros((16, 359, 359))\n",
    "dummy_array4 = np.zeros((16, 359, 359))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally need to extract non-synapses that don't overlap with synapse locations or their cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_cubes(size_cube, array, centroid_locations): ##need to make cubes around our synapses to ensure they don't overlap with\n",
    "##non-synapses\n",
    "\n",
    "    sin=0\n",
    "    for z, y ,x in zip(centroid_locations[:,0], centroid_locations[:,1], centroid_locations[:,2]):\n",
    "        if x-size_cube<0 or x+size_cube>358 or y-size_cube<0 or y+size_cube>358:\n",
    "            continue\n",
    "        for l in range(y-size_cube,y+size_cube): ## 4.84 microns cubed cube is formed by a 22x22 rectangle around the centroid locations\n",
    "            for m in range(x-size_cube,x+size_cube):\n",
    "                array[z][l][m] = 1\n",
    "        sin = sin+1\n",
    "\n",
    "\n",
    "    return  array, sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76411.0\n",
      "68901.0\n",
      "80455.0\n",
      "81192.0\n",
      "187\n",
      "163\n",
      "190\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "a1, s1 =centroid_cubes(11,dummy_array1,ground_truth1)\n",
    "a2, s2 =centroid_cubes(11,dummy_array2,ground_truth2)\n",
    "a3, s3 =centroid_cubes(11,dummy_array3,ground_truth3)\n",
    "a4, s4 =centroid_cubes(11,dummy_array4,ground_truth4)\n",
    "print(np.sum(a1))\n",
    "print(np.sum(a2))\n",
    "print(np.sum(a3))\n",
    "print(np.sum(a4))\n",
    "print(s1)\n",
    "print(s2)\n",
    "print(s3)\n",
    "print(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above cell we had filled our dummy arrays with ones where there is a synapse and cubes around them. In the next cell we are trying to find non-synapse centroids from remaining points in our dummy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finding_non_synapses(size_cube, array2, sin):\n",
    "    num = 0\n",
    "    z_loc = []\n",
    "    y_loc = []\n",
    "    x_loc = []\n",
    "\n",
    "    while num < sin:\n",
    "\n",
    "\n",
    "        x_ns = np.random.randint(size_cube,359-size_cube)##discarding points within 5 pixels from boundary of 359x359x16\n",
    "    ##pixels because want to generate a 4.84 cube for mean intensity. Also, x and y values range from 0-358.\n",
    "        y_ns = np.random.randint(size_cube,359-size_cube)\n",
    "        z_ns = np.random.randint(0,16)\n",
    "        if array2[z_ns][y_ns][x_ns] ==1 or array2[z_ns][y_ns-size_cube][x_ns-size_cube]==1 or array2[z_ns][y_ns+size_cube][x_ns+size_cube]==1 or array2[z_ns][y_ns-size_cube][x_ns+size_cube]==1 or array2[z_ns][y_ns+size_cube][x_ns-size_cube]==1:\n",
    "            continue\n",
    "        z_loc.append(z_ns)\n",
    "        y_loc.append(y_ns)\n",
    "        x_loc.append(x_ns)\n",
    "        for s in range(y_ns-size_cube,y_ns+size_cube): ## 4.84 microns cubed cube is formed by a 10x10 rectangle around the centroid locations\n",
    "            for p in range(x_ns-size_cube,x_ns+size_cube):\n",
    "                 array2[z_ns][s][p] = 1\n",
    "        num += 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return np.asarray((z_loc,y_loc,x_loc)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187, 3)\n",
      "(163, 3)\n",
      "(190, 3)\n",
      "(196, 3)\n"
     ]
    }
   ],
   "source": [
    "non_syn1 = finding_non_synapses(11,a1,s1)\n",
    "print(non_syn1.shape)\n",
    "\n",
    "non_syn2 = finding_non_synapses(11,a2,s2)\n",
    "print(non_syn2.shape)\n",
    "\n",
    "non_syn3 = finding_non_synapses(11,a3,s3)\n",
    "print(non_syn3.shape)\n",
    "\n",
    "non_syn4 = finding_non_synapses(11,a4,s4)\n",
    "print(non_syn4.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the image tiff stack \n",
    "Give the correct paths to the four timepoints image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = tifffile.imread('../dmannan1/LDA2/Substacks_with_SpineROIs/R04_tp1_substack (17-32).tif')\n",
    "im2 = tifffile.imread('../dmannan1/LDA2/Substacks_with_SpineROIs/R04_tp2_substack (17-32).tif')\n",
    "im3 = tifffile.imread('../dmannan1/LDA2/Substacks_with_SpineROIs/R04_tp3_substack (19-34).tif')\n",
    "im4 = tifffile.imread('../dmannan1/LDA2/Substacks_with_SpineROIs/R04_tp4_substack (20-35).tif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally finding the mean intensity of the cubes around the centroids generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [1. 2. 3.]]\n",
      "[[3. 4. 1.]\n",
      " [5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "z = np.zeros((2,3))\n",
    "print(z)\n",
    "z[1] = [1,2,3]\n",
    "print(z)\n",
    "b = np.array([[3,4,1],[5,6,7]])\n",
    "b.shape\n",
    "for idx, val in enumerate(b):\n",
    "    z[idx] = val\n",
    "    \n",
    "print(z)\n",
    "   \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_intensity(cube_size, samples, image, sin):\n",
    "    all_intensity = []\n",
    "    b = 0\n",
    "    loc = np.zeros((sin,3))\n",
    "    \n",
    "\n",
    "\n",
    "    for z, y, x in zip(samples[:,0], samples[:,1], samples[:,2]):##take the z,x,y locations to iterate over\n",
    "        intensity = []\n",
    "        if x-cube_size<0 or x+cube_size>358 or y-cube_size<0 or y+cube_size>358:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        for l in range(y-cube_size,y+cube_size): ## 1 microns cubed cube is formed by a 22x22 rectangle around the centroid locations\n",
    "            for m in range(x-cube_size,x+cube_size):\n",
    "                intensity.append(image[z,l,m])\n",
    "\n",
    "        mean_intensity = np.mean(intensity)\n",
    "        all_intensity.append(mean_intensity)\n",
    "        loc[b] = [z,y,x]\n",
    "        \n",
    "        \n",
    "        b = b+1\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    return np.array(all_intensity).reshape((sin,1)), loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating intensities for synapses and non-synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187, 1)\n",
      "(187, 3)\n",
      "(163, 1)\n",
      "(163, 3)\n",
      "(190, 1)\n",
      "(190, 3)\n",
      "(196, 1)\n",
      "(196, 3)\n"
     ]
    }
   ],
   "source": [
    "### First we'll generate synapse intensities:\n",
    "\n",
    "syn_in1, loc1 = generating_intensity(11,ground_truth1,im1,s1)\n",
    "print(syn_in1.shape)\n",
    "print(loc1.shape)\n",
    "\n",
    "\n",
    "syn_in2, loc2 = generating_intensity(11,ground_truth2,im2,s2)\n",
    "print(syn_in2.shape)\n",
    "print(loc2.shape)\n",
    "\n",
    "\n",
    "\n",
    "syn_in3, loc3 = generating_intensity(11,ground_truth3,im3,s3)\n",
    "print(syn_in3.shape)\n",
    "print(loc3.shape)\n",
    "\n",
    "\n",
    "\n",
    "syn_in4, loc4  = generating_intensity(11,ground_truth4,im4,s4)\n",
    "print(syn_in4.shape)\n",
    "print(loc4.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187, 1)\n",
      "(187, 3)\n",
      "(163, 1)\n",
      "(163, 3)\n",
      "(190, 1)\n",
      "(190, 3)\n",
      "(196, 1)\n",
      "(196, 3)\n"
     ]
    }
   ],
   "source": [
    "## Now onto non-synapse intensities:\n",
    "\n",
    "non_syn_in1, loca  = generating_intensity(11,non_syn1,im1,s1)\n",
    "print(non_syn_in1.shape)\n",
    "print(loca.shape)\n",
    "\n",
    "\n",
    "\n",
    "non_syn_in2, locb = generating_intensity(11,non_syn2,im2,s2)\n",
    "print(non_syn_in2.shape)\n",
    "print(locb.shape)\n",
    "\n",
    "\n",
    "non_syn_in3, locc = generating_intensity(11,non_syn3,im3,s3)\n",
    "print(non_syn_in3.shape)\n",
    "print(locc.shape)\n",
    "\n",
    "\n",
    "non_syn_in4, locd = generating_intensity(11,non_syn4,im4,s4)\n",
    "print(non_syn_in4.shape)\n",
    "print(locd.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the intensity arrays into training and testing:\n",
    "Decide how much the intensity set will be split into testing and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data (syn_intensity, non_syn_intensity, split):\n",
    "    \n",
    "\n",
    "    ##Choosing 50% of the data to be training set\n",
    "    train_syn = syn_intensity[:int(split/2)]\n",
    "    train_non_syn = non_syn_intensity[:int(split/2)]\n",
    "\n",
    "\n",
    "\n",
    "    ##Testing set, take next rows \n",
    "    test_syn= syn_intensity[int(split/2):]\n",
    "    test_non_syn = non_syn_intensity[int(split/2):]\n",
    "\n",
    "\n",
    "\n",
    "    ### The final training set:\n",
    "    train_data = np.vstack((train_syn, train_non_syn))\n",
    "    test_data = np.vstack((test_syn, test_non_syn))\n",
    "    ### The final label generation, synapses belong to class 1, non-synpases belong to class 0:\n",
    "    train_labels = np.append(np.ones((len(train_syn))), np.zeros(len(train_non_syn)))\n",
    "    \n",
    "    return train_data, train_labels, test_data \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data set for the different time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Timepoint 1\n",
    "\n",
    "train1, labels1, test1 = split_data(syn_in1, non_syn_in1, s1)\n",
    "\n",
    "print(s1)\n",
    "print(train1.shape)\n",
    "print(labels1.shape)\n",
    "print(test1.shape)\n",
    "shp_1 = test1.shape[0]\n",
    "print(shp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Timepoint 2\n",
    "\n",
    "train2, labels2, test2 = split_data(syn_in2, non_syn_in2, s2)\n",
    "\n",
    "print(s2)\n",
    "print(train2.shape)\n",
    "print(labels2.shape)\n",
    "print(test2.shape)\n",
    "shp_2 = test2.shape[0]\n",
    "print(shp_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Timepoint 3\n",
    "\n",
    "train3, labels3, test3 = split_data(syn_in3, non_syn_in3, s3)\n",
    "\n",
    "print(s3)\n",
    "print(train3.shape)\n",
    "print(labels3.shape)\n",
    "print(test3.shape)\n",
    "\n",
    "shp_3 = test3.shape[0]\n",
    "print(shp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Timepoint 4\n",
    "\n",
    "train4, labels4, test4 = split_data(syn_in4, non_syn_in4, s4)\n",
    "\n",
    "print(s4)\n",
    "print(train4.shape)\n",
    "print(labels4.shape)\n",
    "print(test4.shape)\n",
    "\n",
    "shp_4 = test4.shape[0]\n",
    "print(shp_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lda(train, labels, test, spl):\n",
    "    b = []\n",
    "    p = []\n",
    "    r = []\n",
    "    f = []\n",
    "    prediction_int_overlays = []\n",
    "    for i in np.arange(0.0, 1.0, 0.1):\n",
    "\n",
    "        lda = LDA().fit(train, labels)\n",
    "        probs_positive_class = lda.predict_proba(test)[:, 1]\n",
    "\n",
    "        prediction = probs_positive_class > i\n",
    "        prediction_int = prediction.astype(int)\n",
    "\n",
    "        spl = int((test.shape[0])/2)\n",
    "        true_pos = np.sum(prediction_int[:spl])\n",
    "        false_pos = np.sum(prediction_int[spl:])\n",
    "        false_neg = spl - true_pos \n",
    "\n",
    "\n",
    "        precision = (true_pos)/(true_pos + false_pos)\n",
    "\n",
    "\n",
    "        recall = (true_pos)/(true_pos+false_neg)\n",
    "\n",
    "        if precision and recall != 0:\n",
    "            f1 = (2*precision*recall)/(precision + recall)\n",
    "        else:\n",
    "            f1 = 0 \n",
    "        print(prediction_int, \"Total wrong synapses:\", false_neg, \"Total wrong non-synapses:\", false_pos, \"Precision:\",\n",
    "             precision, \"Recall:\", recall, \"F1\", f1)\n",
    "        p.append(precision)\n",
    "        r.append(recall)\n",
    "        b.append(i)\n",
    "        f.append(f1)\n",
    "        prediction_int_overlays.append(prediction_int)\n",
    "        \n",
    "    return b, p, r, f, prediction_int_overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_plot(threshold, pres, recall, f1):\n",
    "    plt.plot(threshold, pres)\n",
    "    plt.plot(threshold, recall)\n",
    "    plt.plot(threshold, f1)\n",
    "    plt.xlabel('Decision boundary threshold')\n",
    "    plt.ylabel('Value of the labelled metric')\n",
    "\n",
    "    plt.legend(['Precision', 'Recall', \"F1\"], loc='upper right')\n",
    "    plt.title('Mean intensity of a 4.84 ${\\mu}m^3$ cube and LDA performance by varying decision threshold')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-96778ce811ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshp_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstat_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "t1, p1, r1, f1, pred1 = lda(train1,labels1,test1, shp_1)\n",
    "stat_plot(t1, p1, r1, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2, p2, r2, f2, pred2= lda(train2, labels2, test2, shp_2)\n",
    "stat_plot(t2, p2, r2, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t3, p3, r3, f3, pred3 = lda(train3, labels3, test3, shp_3)\n",
    "stat_plot(t3, p3, r3, f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t4, p4, r4, f4, pred4 = lda(train4,labels4,test4, shp_4)\n",
    "stat_plot(t4, p4, r4, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding centroid locations for tracking:\n",
    "Pick centroids predicted as synapses at a conservative threshold, here we pick 0.3 decision boundary for all the timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "syn_locations = master_data[120:340,4:7]\n",
    "z_array=[]\n",
    "y_array=[]\n",
    "x_array=[]\n",
    "\n",
    "for idx, val in enumerate(prediction_int_overlays[6][220:]):\n",
    "    if val==1:\n",
    "        z_array.append(non_syn_locations[idx][0])\n",
    "        y_array.append(non_syn_locations[idx][1])\n",
    "        x_array.append(non_syn_locations[idx][2])\n",
    "data={'Z':z_array,'Y':y_array,'X':x_array}\n",
    "void=pd.DataFrame(data,columns=['Z','Y','X'])\n",
    "void.to_csv('0.6_decision_4.84_cubes.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LDA()\n",
    "clf.fit(train1, labels1)\n",
    "#predict_proba(test_data)\n",
    "j = clf.predict(test1)\n",
    "print(j)\n",
    "print(clf.predict_proba(test1))\n",
    "print(clf.predict_proba(test1)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_positive_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(j[148:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:LIDS]",
   "language": "python",
   "name": "conda-env-LIDS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
